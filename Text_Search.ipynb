{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise: Building and Loading Text Search in Python Whoosh\n",
    "\n",
    "\n",
    "## OUTLINE\n",
    " 1. [Whoosh](#Whoosh_text)\n",
    " 1. [Task at hand](#task)\n",
    " 1. [Buiding our Whoosh Schema](#build_it)\n",
    " 1. [Loading Data](#load_it)\n",
    " 1. [Executing Queries, Google-lite...very very lite](#search_me) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='Whoosh_text' ></a>\n",
    "\n",
    "## Whoosh\n",
    "\n",
    "Whoosh was started as a quick and dirty search server for the online documentation of the Houdini 3D animation software package. \n",
    "Side Effects Software generously allowed the code to be open source, in case it might be useful to anyone else who needs a very flexible or pure-Python search engine (or both!).\n",
    "\n",
    "  * Whoosh is fast, but uses only pure Python, so it will run anywhere Python runs, without requiring a compiler.\n",
    "  * By default, Whoosh uses the Okapi BM25F ranking function, but like most things the ranking function can be easily customized.\n",
    "  * Whoosh creates fairly small indexes compared to many other search libraries.\n",
    "  * All indexed text in Whoosh must be unicode.\n",
    "  * Whoosh lets you store arbitrary Python objects with indexed documents.\n",
    "\n",
    "### What is Whoosh?\n",
    "\n",
    "Whoosh is a fast, pure Python search engine library.\n",
    "\n",
    "The primary design impetus of Whoosh is that it is pure Python. \n",
    "You should be able to use Whoosh anywhere you can use Python, no compiler or Java required.\n",
    "\n",
    "Like one of its ancestors, Lucene, Whoosh is not really a search engine, itâ€™s a programmer library for creating a search engine.\n",
    "\n",
    "Practically no important behavior of Whoosh is hard-coded. \n",
    "Indexing of text, the level of information stored for each term in each field, parsing of search queries, the types of queries allowed, scoring algorithms, etc. are all customizable, replaceable, and extensible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='task' ></a>\n",
    "\n",
    "## Task at hand\n",
    "\n",
    "For this exercise, we are going to walk through the process of creating full text search capability within Python for integration into other analytical processes.\n",
    "\n",
    "You previously read about the _`book`_ data and you have seen the data used for a corpus in a PostgreSQL full text search, as well as using Whoosh in Python.\n",
    "\n",
    "Now, we are going go through the similar process to build a search engine in pure Python for a different corpus.\n",
    "\n",
    "The process will take very little time and the useability of the full text search is multiplied by degree of heterogeneous data that can be integrated with the full text search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='build_it' ></a>\n",
    "\n",
    "## Buiding our Whoosh Schema\n",
    "\n",
    "Recall, the `book/` folder is composed of a collection of text files, each its own book chapter.\n",
    "\n",
    "In whoosh, we structure the retrieval system by defining a storage schema.\n",
    "\n",
    "From the lab with the text files:\n",
    "```\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer())\n",
    "                )\n",
    "```\n",
    "\n",
    "This tells us we are defining records to have a `(filename, content)`\n",
    "\n",
    "For this exercise, we will be using a few Wikipedia pages for our data source.\n",
    "\n",
    "### 1) For this exercise, you should look at a few of these web pages:\n",
    "\n",
    "  * https://en.wikipedia.org/wiki/Nyctimantis\n",
    "  * https://en.wikipedia.org/wiki/Osteocephalus\n",
    "  * https://en.wikipedia.org/wiki/Osteopilus\n",
    "  \n",
    "Specifically, inspect the HTML source and the \n",
    "```HTML\n",
    "<table class=\"infobox biota\" ... </table>\n",
    "```\n",
    "\n",
    "You need to extend the schema definition to collect the table data when available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\n",
    "from whoosh.analysis import StemmingAnalyzer\n",
    "\n",
    "schema = Schema(filename=ID(stored=True),\n",
    "                content=TEXT(analyzer=StemmingAnalyzer()),\n",
    "                # Extend the schema definition to capture relevant table data\n",
    "                kingdom=TEXT(stored=True),\n",
    "                phylum=TEXT(stored=True),\n",
    "                class_=TEXT(stored=True),\n",
    "                order=TEXT(stored=True),\n",
    "                family=TEXT(stored=True),\n",
    "                subfamily=TEXT(stored=True),\n",
    "                genus=TEXT(stored=True)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='load_it' ></a>\n",
    "\n",
    "## Loading Data\n",
    "\n",
    "For this exercise, we have created a small folder of a few Wikipedia pages under the `en.wikipedia.org/wiki` folder:\n",
    "\n",
    "```Bash\n",
    "[scottgs@metal exercises]$ ls en.wikipedia.org/wiki\n",
    "Acris.html           Ecnomiohyla.html      Myersiohyla.html    Scinax.html\n",
    "Anotheca.html        Exerodonta.html       Nyctimantis.html    Smilisca.html\n",
    "Aparasphenodon.html  Hyla.html             Osteocephalus.html  Sphaenorhynchus.html\n",
    "Aplastodiscus.html   Hylidae.html          Osteopilus.html     Tepuihyla.html\n",
    "Argenteohyla.html    Hylinae.html          Phyllodytes.html    Tlalocohyla.html\n",
    "Bokermannohyla.html  Hyloscirtus.html      Phytotriades.html   Trachycephalus.html\n",
    "Bromeliohyla.html    Hypsiboas.html        Plectrohyla.html    Triprion.html\n",
    "Charadrahyla.html    Isthmohyla.html       Pseudacris.html     Xenohyla.html\n",
    "Corythomantis.html   Itapotihyla.html      Pseudis.html\n",
    "Dendropsophus.html   Lysapsus.html         Ptychohyla.html\n",
    "Duellmanohyla.html   Megastomatohyla.html  Scarthyla.html\n",
    "\n",
    "```\n",
    "You will create the _whoosh_ index files in the folder then ingest the files.\n",
    "\n",
    "To load the data, a python script with follow the basic crawling behavior\n",
    "\n",
    " 1. For each file/folder in the specified starting folder:\n",
    " 1. If it is a folder, recurse into folder and process contents\n",
    " 1. If it is a file, read contents and load into indexer.\n",
    " \n",
    "## Follow the lab for Python IR with whoosh to complete this exercise.\n",
    "\n",
    "### Step 2) Create / Initialize the whoosh index and get the `writer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "from whoosh import index\n",
    "\n",
    "# Step 2 below this comment\"\n",
    "ix = index.create_in(\"en.wikipedia.org/wiki\", schema)\n",
    "writer = ix.writer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bs4\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=2, depth=1, width=80, compact=True)\n",
    "\n",
    "def pullBiota(content):\n",
    "    '''\n",
    "    Content is the HTML content\n",
    "    '''\n",
    "     # Start up a dictionary\n",
    "    data = {}\n",
    "\n",
    "    soup = bs4.BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    # TODO: Process the \"<table class=\"infobox biota\" ... </table> data\n",
    "    biota_table = soup.findAll('table', attrs={'class': re.compile(r'\\binfobox\\b')} )\n",
    "    # Now that we have pulled the table, lets process the rows\n",
    "    table_as_bs = bs4.BeautifulSoup(str(biota_table), 'html.parser')\n",
    "\n",
    "    for row in table_as_bs.findAll('tr'):\n",
    "        # Each row has td or th elements, we know we need the td \n",
    "        cells = row.findAll('td')\n",
    "        # Only the two-column rows matter\n",
    "        if (len(cells) == 2):\n",
    "            # print(cells[0].string,'-',cells[1].string)\n",
    "            data[cells[0].string.strip(':')] = cells[1].string\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Genus': None, 'Class': 'Amphibia', 'Phylum': 'Chordata', 'Order': 'Anura', 'Family': 'Hylidae', 'Kingdom': 'Animalia', 'Subfamily': 'Hylinae'}\n"
     ]
    }
   ],
   "source": [
    "with open(\"../exercises/en.wikipedia.org/wiki/Tepuihyla.html\", 'r', encoding=\"utf-8\") as infile:\n",
    "    content=infile.read()\n",
    "    tableOut = pullBiota(content)\n",
    "    print(tableOut)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Adapt the helper functions\n",
    "\n",
    "Note the subtle changes.\n",
    "Please adapt the code below such as provided recursive parsing of the HTML (.html) files, indexing with the Whoosh API.\n",
    "Trust no code, verify all code segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visible(element):\n",
    "    if element.parent.name in ['style', 'script', '[document]', 'head', 'title']:\n",
    "        return False\n",
    "    elif re.match('<!--.*-->', str(element)):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def loadFile(writer, fname):\n",
    "    '''\n",
    "    Read file contents, load into database.\n",
    "    '''\n",
    "    with open(fname, 'r',encoding=\"utf-8\") as infile:\n",
    "        content=infile.read()\n",
    "        tableOut=pullBiota(content)\n",
    "        soup = bs4.BeautifulSoup(content, 'html.parser')\n",
    "        texts = soup.findAll(text=True)\n",
    "        \n",
    "        # Process all the visible text\n",
    "        visible_texts = filter(visible, texts)\n",
    "        # TODO: Assemble all visible_texts into a content string\n",
    "        visible_content = \"\"\n",
    "        for i in visible_texts:\n",
    "            visible_content += \" \" + i.strip('\\n')\n",
    "        \n",
    "        # TODO: Process the \"<table class=\"infobox biota\" ... </table> data\n",
    "        \n",
    "        writer.add_document(filename=fname,\n",
    "                           content=visible_content,\n",
    "                           kingdom=tableOut.get('Kingdom'),\n",
    "                           phylum=tableOut.get('Phylum'),\n",
    "                           class_=tableOut.get('Class'),\n",
    "                           order=tableOut.get('Order'),\n",
    "                           family=tableOut.get('Family'),\n",
    "                           subfamily=tableOut.get('Subfamily'),\n",
    "                           genus=tableOut.get('Genus')\n",
    "                          ) \n",
    "        \n",
    "        # Write to the index\n",
    "        print(\"Indexed: \", fname)\n",
    "\n",
    "def processFolder(writer,folder):\n",
    "    '''\n",
    "    Process a folder for files and subfolders\n",
    "    '''\n",
    "    print('Processing folder: ',folder)\n",
    "    for root, dirs, files in os.walk(folder):\n",
    "        print(\"root = \", root)\n",
    "        # Process Files\n",
    "        for file in files:\n",
    "            if file.endswith(\".html\"):\n",
    "                filename = os.path.join(root, file)\n",
    "                print('Processing File:',filename)\n",
    "                loadFile(writer,filename)\n",
    "            else:\n",
    "                print(\"Unhandled File\")\n",
    "        # Recurse into subfolders\n",
    "        for d in dirs:\n",
    "            print(\"recursing into \",d)\n",
    "            processFolder(writer,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Parse with our defined functions in place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder:  en.wikipedia.org/wiki\n",
      "root =  en.wikipedia.org/wiki\n",
      "Processing File: en.wikipedia.org/wiki/Acris.html\n",
      "Indexed:  en.wikipedia.org/wiki/Acris.html\n",
      "Processing File: en.wikipedia.org/wiki/Anotheca.html\n",
      "Indexed:  en.wikipedia.org/wiki/Anotheca.html\n",
      "Processing File: en.wikipedia.org/wiki/Aparasphenodon.html\n",
      "Indexed:  en.wikipedia.org/wiki/Aparasphenodon.html\n",
      "Processing File: en.wikipedia.org/wiki/Aplastodiscus.html\n",
      "Indexed:  en.wikipedia.org/wiki/Aplastodiscus.html\n",
      "Processing File: en.wikipedia.org/wiki/Argenteohyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Argenteohyla.html\n",
      "Processing File: en.wikipedia.org/wiki/Bokermannohyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Bokermannohyla.html\n",
      "Processing File: en.wikipedia.org/wiki/Bromeliohyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Bromeliohyla.html\n",
      "Processing File: en.wikipedia.org/wiki/Charadrahyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Charadrahyla.html\n",
      "Processing File: en.wikipedia.org/wiki/Corythomantis.html\n",
      "Indexed:  en.wikipedia.org/wiki/Corythomantis.html\n",
      "Processing File: en.wikipedia.org/wiki/Dendropsophus.html\n",
      "Indexed:  en.wikipedia.org/wiki/Dendropsophus.html\n",
      "Processing File: en.wikipedia.org/wiki/Duellmanohyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Duellmanohyla.html\n",
      "Processing File: en.wikipedia.org/wiki/Ecnomiohyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Ecnomiohyla.html\n",
      "Processing File: en.wikipedia.org/wiki/Exerodonta.html\n",
      "Indexed:  en.wikipedia.org/wiki/Exerodonta.html\n",
      "Processing File: en.wikipedia.org/wiki/Hyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Hyla.html\n",
      "Processing File: en.wikipedia.org/wiki/Hylidae.html\n",
      "Indexed:  en.wikipedia.org/wiki/Hylidae.html\n",
      "Processing File: en.wikipedia.org/wiki/Hylinae.html\n",
      "Indexed:  en.wikipedia.org/wiki/Hylinae.html\n",
      "Processing File: en.wikipedia.org/wiki/Hyloscirtus.html\n",
      "Indexed:  en.wikipedia.org/wiki/Hyloscirtus.html\n",
      "Processing File: en.wikipedia.org/wiki/Hypsiboas.html\n",
      "Indexed:  en.wikipedia.org/wiki/Hypsiboas.html\n",
      "Processing File: en.wikipedia.org/wiki/Isthmohyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Isthmohyla.html\n",
      "Processing File: en.wikipedia.org/wiki/Itapotihyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Itapotihyla.html\n",
      "Processing File: en.wikipedia.org/wiki/Lysapsus.html\n",
      "Indexed:  en.wikipedia.org/wiki/Lysapsus.html\n",
      "Processing File: en.wikipedia.org/wiki/Megastomatohyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Megastomatohyla.html\n",
      "Processing File: en.wikipedia.org/wiki/Myersiohyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Myersiohyla.html\n",
      "Processing File: en.wikipedia.org/wiki/Nyctimantis.html\n",
      "Indexed:  en.wikipedia.org/wiki/Nyctimantis.html\n",
      "Processing File: en.wikipedia.org/wiki/Osteocephalus.html\n",
      "Indexed:  en.wikipedia.org/wiki/Osteocephalus.html\n",
      "Processing File: en.wikipedia.org/wiki/Osteopilus.html\n",
      "Indexed:  en.wikipedia.org/wiki/Osteopilus.html\n",
      "Processing File: en.wikipedia.org/wiki/Phyllodytes.html\n",
      "Indexed:  en.wikipedia.org/wiki/Phyllodytes.html\n",
      "Processing File: en.wikipedia.org/wiki/Phytotriades.html\n",
      "Indexed:  en.wikipedia.org/wiki/Phytotriades.html\n",
      "Processing File: en.wikipedia.org/wiki/Plectrohyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Plectrohyla.html\n",
      "Processing File: en.wikipedia.org/wiki/Pseudacris.html\n",
      "Indexed:  en.wikipedia.org/wiki/Pseudacris.html\n",
      "Processing File: en.wikipedia.org/wiki/Pseudis.html\n",
      "Indexed:  en.wikipedia.org/wiki/Pseudis.html\n",
      "Processing File: en.wikipedia.org/wiki/Ptychohyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Ptychohyla.html\n",
      "Processing File: en.wikipedia.org/wiki/Scarthyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Scarthyla.html\n",
      "Processing File: en.wikipedia.org/wiki/Scinax.html\n",
      "Indexed:  en.wikipedia.org/wiki/Scinax.html\n",
      "Processing File: en.wikipedia.org/wiki/Smilisca.html\n",
      "Indexed:  en.wikipedia.org/wiki/Smilisca.html\n",
      "Processing File: en.wikipedia.org/wiki/Sphaenorhynchus.html\n",
      "Indexed:  en.wikipedia.org/wiki/Sphaenorhynchus.html\n",
      "Processing File: en.wikipedia.org/wiki/Tepuihyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Tepuihyla.html\n",
      "Processing File: en.wikipedia.org/wiki/Tlalocohyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Tlalocohyla.html\n",
      "Processing File: en.wikipedia.org/wiki/Trachycephalus.html\n",
      "Indexed:  en.wikipedia.org/wiki/Trachycephalus.html\n",
      "Processing File: en.wikipedia.org/wiki/Triprion.html\n",
      "Indexed:  en.wikipedia.org/wiki/Triprion.html\n",
      "Processing File: en.wikipedia.org/wiki/Xenohyla.html\n",
      "Indexed:  en.wikipedia.org/wiki/Xenohyla.html\n",
      "Unhandled File\n",
      "Unhandled File\n",
      "Unhandled File\n",
      "Unhandled File\n",
      "Unhandled File\n",
      "recursing into  .ipynb_checkpoints\n",
      "Processing folder:  .ipynb_checkpoints\n",
      "root =  .ipynb_checkpoints\n",
      "Unhandled File\n",
      "recursing into  MAIN.tmp\n",
      "Processing folder:  MAIN.tmp\n",
      "root =  en.wikipedia.org/wiki/.ipynb_checkpoints\n",
      "root =  en.wikipedia.org/wiki/MAIN.tmp\n",
      "Unhandled File\n"
     ]
    }
   ],
   "source": [
    "# Start processing the folder and commit the work\n",
    "# ---------------------------------------------------\n",
    "processFolder(writer,\"en.wikipedia.org/wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writer.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "<a id='search_me' ></a>\n",
    "\n",
    "### 5) Executing Queries\n",
    "\n",
    "Read: \n",
    "  http://whoosh.readthedocs.io/en/latest/searching.html\n",
    "  \n",
    "Previously, we hard-coded query strings into the code cells.\n",
    "\n",
    "Now, use the `input()` function collect a query string from the user. \n",
    "Then execute the search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Tepuihyla.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Hylinae.html'}>\n",
      "<Hit {'order': 'Anura', 'filename': 'en.wikipedia.org/wiki/Hylidae.html', 'phylum': 'Chordata', 'kingdom': 'Animalia', 'class_': 'Amphibia'}>\n"
     ]
    }
   ],
   "source": [
    "from whoosh.qparser import QueryParser\n",
    "\n",
    "# Write your code below this comment:\n",
    "# --------------------------------------\n",
    "\n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(u\"Tepuihyla\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What would you like to search for..?Chordata\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Scarthyla.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Aplastodiscus.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Corythomantis.html'}>\n",
      "<Hit {'order': 'Anura', 'filename': 'en.wikipedia.org/wiki/Hylidae.html', 'phylum': 'Chordata', 'kingdom': 'Animalia', 'class_': 'Amphibia'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Lysapsus.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Anotheca.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Triprion.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Charadrahyla.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Nyctimantis.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Tlalocohyla.html'}>\n"
     ]
    }
   ],
   "source": [
    "user_search_string = input(\"What would you like to search for..?\")\n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(user_search_string)\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Write example queries to ensure you can search the index \n",
    "\n",
    "```HTML\n",
    "<table class=\"infobox biota\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Acris.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Anotheca.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Aparasphenodon.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Aplastodiscus.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Argenteohyla.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Bokermannohyla.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Bromeliohyla.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Charadrahyla.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Corythomantis.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Dendropsophus.html'}>\n"
     ]
    }
   ],
   "source": [
    "# Write your code below this comment:\n",
    "# --------------------------------------\n",
    "qp = QueryParser(\"order\", schema=ix.schema)\n",
    "q = qp.parse(u\"Anura\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What would you like to search for..?Animalia\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Scarthyla.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Aplastodiscus.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Corythomantis.html'}>\n",
      "<Hit {'order': 'Anura', 'filename': 'en.wikipedia.org/wiki/Hylidae.html', 'phylum': 'Chordata', 'kingdom': 'Animalia', 'class_': 'Amphibia'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Lysapsus.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Anotheca.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Triprion.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Charadrahyla.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Nyctimantis.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Tlalocohyla.html'}>\n"
     ]
    }
   ],
   "source": [
    "user_search_string = input(\"What would you like to search for..?\")\n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(user_search_string)\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Acris.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Anotheca.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Aparasphenodon.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Aplastodiscus.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Argenteohyla.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Bokermannohyla.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Bromeliohyla.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Charadrahyla.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Corythomantis.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Dendropsophus.html'}>\n"
     ]
    }
   ],
   "source": [
    "# Write your code below this comment:\n",
    "# --------------------------------------\n",
    "qp = QueryParser(\"phylum\", schema=ix.schema)\n",
    "q = qp.parse(u\"Chordata\")\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What would you like to search for..?phylum\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Scarthyla.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Aplastodiscus.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Corythomantis.html'}>\n",
      "<Hit {'order': 'Anura', 'filename': 'en.wikipedia.org/wiki/Hylidae.html', 'phylum': 'Chordata', 'kingdom': 'Animalia', 'class_': 'Amphibia'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Lysapsus.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Anotheca.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Triprion.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'filename': 'en.wikipedia.org/wiki/Charadrahyla.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Nyctimantis.html'}>\n",
      "<Hit {'kingdom': 'Animalia', 'phylum': 'Chordata', 'class_': 'Amphibia', 'order': 'Anura', 'family': 'Hylidae', 'subfamily': 'Hylinae', 'filename': 'en.wikipedia.org/wiki/Tlalocohyla.html'}>\n"
     ]
    }
   ],
   "source": [
    "user_search_string = input(\"What would you like to search for..?\")\n",
    "qp = QueryParser(\"content\", schema=ix.schema)\n",
    "q = qp.parse(user_search_string)\n",
    "\n",
    "with ix.searcher() as s:\n",
    "    results = s.search(q)\n",
    "    for hit in results:\n",
    "        print(hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE YOUR NOTEBOOK WITH ALL EXECUTED CELLS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
